{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Data Cleaning </center></h1>\n",
    "\n",
    "<img src=https://analyticsindiamag.com/wp-content/uploads/2018/01/data-cleaning-1280x720.png style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from autocorrect import spell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Person</th>\n",
       "      <th>VideoId</th>\n",
       "      <th>VideoTitle</th>\n",
       "      <th>Description</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Dislikes</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Emily Ann Shaheen</td>\n",
       "      <td>2mg3sFuiwRw</td>\n",
       "      <td>HOW TO BE AN ARAB GIRL!</td>\n",
       "      <td>Heyyy guys! Thank you so much for watching! Th...</td>\n",
       "      <td>80408</td>\n",
       "      <td>1971</td>\n",
       "      <td>147</td>\n",
       "      <td>640</td>\n",
       "      <td>Im Arab from iraq but my golden name is in eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Emily Ann Shaheen</td>\n",
       "      <td>Wlub9KOJBt4</td>\n",
       "      <td>ARAB GIRL STEREOTYPES!</td>\n",
       "      <td>Thanks for watching babes! xx, Emily Ann Shahe...</td>\n",
       "      <td>27925</td>\n",
       "      <td>608</td>\n",
       "      <td>46</td>\n",
       "      <td>240</td>\n",
       "      <td>Being an Arab is great and proud also true we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nowthisisliving</td>\n",
       "      <td>D2iCOMoOkyI</td>\n",
       "      <td>LESBIAN INTERVIEWS EX BOYFRIEND</td>\n",
       "      <td>please be kind in the comments, this boy is th...</td>\n",
       "      <td>2098024</td>\n",
       "      <td>51987</td>\n",
       "      <td>1644</td>\n",
       "      <td>2718</td>\n",
       "      <td>My ex broke up with me because she wanted to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nowthisisliving</td>\n",
       "      <td>_Uxw2X0hNGg</td>\n",
       "      <td>why we broke up</td>\n",
       "      <td>I know this is a tough video for everyone. We ...</td>\n",
       "      <td>3081184</td>\n",
       "      <td>74616</td>\n",
       "      <td>1766</td>\n",
       "      <td>9406</td>\n",
       "      <td>still high key want them to get back together ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Madison Beer</td>\n",
       "      <td>-9BfaW69LSk</td>\n",
       "      <td>Madison Beer- Catch Me Cover</td>\n",
       "      <td>HEY YOUTUBE!!!!!!!!! LONG TIME NO VIDEO! so so...</td>\n",
       "      <td>920561</td>\n",
       "      <td>14418</td>\n",
       "      <td>2408</td>\n",
       "      <td>2747</td>\n",
       "      <td>Be strong That's my fav song of demi lovato I'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Person      VideoId                       VideoTitle  \\\n",
       "0  Emily Ann Shaheen  2mg3sFuiwRw          HOW TO BE AN ARAB GIRL!   \n",
       "1  Emily Ann Shaheen  Wlub9KOJBt4           ARAB GIRL STEREOTYPES!   \n",
       "2    nowthisisliving  D2iCOMoOkyI  LESBIAN INTERVIEWS EX BOYFRIEND   \n",
       "3    nowthisisliving  _Uxw2X0hNGg                  why we broke up   \n",
       "4       Madison Beer  -9BfaW69LSk     Madison Beer- Catch Me Cover   \n",
       "\n",
       "                                         Description ViewCount  Likes  \\\n",
       "0  Heyyy guys! Thank you so much for watching! Th...     80408   1971   \n",
       "1  Thanks for watching babes! xx, Emily Ann Shahe...     27925    608   \n",
       "2  please be kind in the comments, this boy is th...   2098024  51987   \n",
       "3  I know this is a tough video for everyone. We ...   3081184  74616   \n",
       "4  HEY YOUTUBE!!!!!!!!! LONG TIME NO VIDEO! so so...    920561  14418   \n",
       "\n",
       "  Dislikes CommentCount                                           Comments  \n",
       "0      147          640  Im Arab from iraq but my golden name is in eng...  \n",
       "1       46          240  Being an Arab is great and proud also true we ...  \n",
       "2     1644         2718  My ex broke up with me because she wanted to b...  \n",
       "3     1766         9406  still high key want them to get back together ...  \n",
       "4     2408         2747  Be strong That's my fav song of demi lovato I'...  "
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('videoInfo.pkl')\n",
    "df.rename(columns=\n",
    "          {\"View Count\": \"ViewCount\",\n",
    "           \"Comment Count\": \"CommentCount\",\n",
    "            \"Uploader\":\"Person\"\n",
    "          }, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting only person and comments he or she received from each of the videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Person</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Emily Ann Shaheen</td>\n",
       "      <td>Im Arab from iraq but my golden name is in eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Emily Ann Shaheen</td>\n",
       "      <td>Being an Arab is great and proud also true we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nowthisisliving</td>\n",
       "      <td>My ex broke up with me because she wanted to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nowthisisliving</td>\n",
       "      <td>still high key want them to get back together ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Madison Beer</td>\n",
       "      <td>Be strong That's my fav song of demi lovato I'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Person                                           Comments\n",
       "0  Emily Ann Shaheen  Im Arab from iraq but my golden name is in eng...\n",
       "1  Emily Ann Shaheen  Being an Arab is great and proud also true we ...\n",
       "2    nowthisisliving  My ex broke up with me because she wanted to b...\n",
       "3    nowthisisliving  still high key want them to get back together ...\n",
       "4       Madison Beer  Be strong That's my fav song of demi lovato I'..."
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_comments = df[['Person','Comments']]\n",
    "person_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some videos I chose are uploaded by uploader, but subjects are different in the content. Regardless, I would like to analyze comments in the videos. \n",
    "Hence, I am replacing the person name by subject of the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Person</th>\n",
       "      <td>Emily Ann Shaheen</td>\n",
       "      <td>Emily Ann Shaheen</td>\n",
       "      <td>Shannon Beveridge</td>\n",
       "      <td>Shannon Beveridge</td>\n",
       "      <td>Madison Beer</td>\n",
       "      <td>Madison Beer</td>\n",
       "      <td>Rebecca Black</td>\n",
       "      <td>Rebecca Black</td>\n",
       "      <td>Madeline &amp; Eric</td>\n",
       "      <td>Madeline &amp; Eric</td>\n",
       "      <td>Steven Assanti</td>\n",
       "      <td>Steven Assanti</td>\n",
       "      <td>Danielle Cohn</td>\n",
       "      <td>Danielle Cohn</td>\n",
       "      <td>Richard Gale</td>\n",
       "      <td>Richard Gale</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0                  1                  2   \\\n",
       "Person  Emily Ann Shaheen  Emily Ann Shaheen  Shannon Beveridge   \n",
       "\n",
       "                       3             4             5              6   \\\n",
       "Person  Shannon Beveridge  Madison Beer  Madison Beer  Rebecca Black   \n",
       "\n",
       "                   7                8                9               10  \\\n",
       "Person  Rebecca Black  Madeline & Eric  Madeline & Eric  Steven Assanti   \n",
       "\n",
       "                    11             12             13            14  \\\n",
       "Person  Steven Assanti  Danielle Cohn  Danielle Cohn  Richard Gale   \n",
       "\n",
       "                  15  \n",
       "Person  Richard Gale  "
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Replacing Person with actual subject in the video\n",
    "person_comments = person_comments.replace({'Person' : {'Charlie Ayee': 'Steven Assanti', \n",
    "                               'AlienRadioStation' : 'Steven Assanti',\n",
    "                               'rebecca' : 'Rebecca Black',\n",
    "                               'nowthisisliving':'Shannon Beveridge',\n",
    "                               'RoughChop':'Richard Gale'}})\n",
    "person_comments['Person'].to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining subjects in one row**:\n",
    "\n",
    "Each subject has multiple videos and comment pairs. I am interested in finding what kind of comments each uploader gets. In this light I can combine comments from multiple videos into one for a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Person</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Danielle Cohn</td>\n",
       "      <td>Disgusting So u 13 and you had a first time Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Emily Ann Shaheen</td>\n",
       "      <td>Im Arab from iraq but my golden name is in eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Madeline &amp; Eric</td>\n",
       "      <td>Your grocery carts are sooo small. Ours look g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Madison Beer</td>\n",
       "      <td>Be strong That's my fav song of demi lovato I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rebecca Black</td>\n",
       "      <td>My old girlfriend also had a book titled Knot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Richard Gale</td>\n",
       "      <td>Hes a bitch Richard your a n idiot mate He bul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shannon Beveridge</td>\n",
       "      <td>My ex broke up with me because she wanted to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Steven Assanti</td>\n",
       "      <td>Steven is a Sagittarius, we be great is my guy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Person                                           Comments\n",
       "0      Danielle Cohn  Disgusting So u 13 and you had a first time Th...\n",
       "1  Emily Ann Shaheen  Im Arab from iraq but my golden name is in eng...\n",
       "2    Madeline & Eric  Your grocery carts are sooo small. Ours look g...\n",
       "3       Madison Beer  Be strong That's my fav song of demi lovato I'...\n",
       "4      Rebecca Black  My old girlfriend also had a book titled Knot ...\n",
       "5       Richard Gale  Hes a bitch Richard your a n idiot mate He bul...\n",
       "6  Shannon Beveridge  My ex broke up with me because she wanted to b...\n",
       "7     Steven Assanti  Steven is a Sagittarius, we be great is my guy..."
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_comments_grouped=person_comments.groupby(['Person'])['Comments'].apply(','.join).reset_index()\n",
    "person_comments_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickling grouped unclean data for later use\n",
    "person_comments_grouped.to_pickle(\"person_comments_grouped.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Im Arab from iraq but my golden name is in english 😂 You remind me so much of rclbeauty101.. I hope to marry an Arab woman one day. I never noticed how attractive I find them until I became an adult I love it! I sooooo have a necklace with my name in Arabic and I\\'m very proud of it! Thank you for your videos! Much love to you from another Arab-American \"sister\"! I thought I\\'d get some tips to look like an arabian Girl 😂😂😂😂 I\\'m literally the hairiest fuckin girl in my class(ALL ARABS) and I can\\'t do about it.... except mustacge and brows I’m not Arab I’m Somali \\nMy name is hard to say\\nI have a gold necklace \\nIt says my name on it  \\nLike if you got a gold \\nNecklace Wanna be a arab girl? Hate Men! Abdullah Ahmed more like \\nWanna be a brainwashed SJW? hate men Where did you get your necklace?!💖 Is middle east a new continent \\n Since when the middle east became a new continent its not a continent... Middle East is part of Asia. I\\'m arab(Algeriaaaaaa🇩🇿🇩🇿🇩🇿🇩🇿🇩🇿) and my face is covered with mo'"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at a sample text:\n",
    "person_comments_grouped['Comments'][1][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "\n",
    "**Common data cleaning steps on all text:**\n",
    "\n",
    "* Make text lower case\n",
    "* Remove punctuation\n",
    "* Remove numerical values\n",
    "* Remove common non-sensical text\n",
    "* Tokenize text\n",
    "* Remove stop words\n",
    "* Removing emojis and icons\n",
    "\n",
    "**More data cleaning steps after tokenization:**\n",
    "\n",
    "* Stemming Lemmatization\n",
    "* Parts of speech tagging\n",
    "* Create bi-grams or tri-grams\n",
    "* Deal with typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning\n",
    "\n",
    "def textCleaning(text):\n",
    "    text = re.sub('[%s]' % re.escape(punctuation), ' ', text) #remove punctuation\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text) #remove words with numbers\n",
    "    text = re.sub('[‘’“”…]', ' ', text)\n",
    "    text = text.lower()  # make lowercase\n",
    "    text = re.sub('\\n', ' ', text) #remove new line\n",
    "    return text\n",
    "\n",
    "text_cleaner = lambda x: textCleaning(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Emojis\n",
    "\n",
    "def removing_emojis(text):\n",
    "    # Emojis pattern\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                    u\"\\U00002702-\\U000027B0\"\n",
    "                    u\"\\U000024C2-\\U0001F251\"\n",
    "                    u\"\\U0001f926-\\U0001f937\"\n",
    "                    u'\\U00010000-\\U0010ffff'\n",
    "                    u\"\\u200d\"\n",
    "                    u\"\\u2640-\\u2642\"\n",
    "                    u\"\\u2600-\\u2B55\"\n",
    "                    u\"\\u23cf\"\n",
    "                    u\"\\u23e9\"\n",
    "                    u\"\\u231a\"\n",
    "                    u\"\\u3030\"\n",
    "                    u\"\\ufe0f\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r' ', text) \n",
    "\n",
    "emojiRemoval = lambda x: removing_emojis(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "lemma = WordNetLemmatizer()\n",
    "def lemmatization(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_text=[]\n",
    "    for w in words:\n",
    "        w= lemma.lemmatize(w)\n",
    "        lemmatized_text.append(w)\n",
    "    return ' '.join(lemmatized_text)\n",
    "\n",
    "lemmatizor = lambda x: lemmatization(x)\n",
    "\n",
    "\n",
    "#Stemming\n",
    "porter = PorterStemmer()\n",
    "def stemming(text):\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_text = []\n",
    "    for w in words:\n",
    "        w = porter.stem(w)\n",
    "        stemmed_text.append(w)\n",
    "    return ' '.join(stemmed_text)\n",
    "\n",
    "stemmor = lambda x: stemming(x)\n",
    "\n",
    "\n",
    "#Spell Checking\n",
    "def spellCheck(text):\n",
    "    autocorrected_text = spell(text)\n",
    "    return autocorrected_text\n",
    "\n",
    "spell_checker = lambda x: spellCheck(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing Data \n",
    "\n",
    "\n",
    "### Corpus \n",
    "\n",
    "Corpus is a collection of text. Here we are creating collection of cleaned texts. \n",
    "\n",
    "Let us apply different text cleaning functions we created above.\n",
    "\n",
    "For now, I am skipping lemmatization, stemming and spell checking. The reason for this is because they tend to correct per change spellings of texts. In this project, we are looking for abuse words too. We don't want such words to be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Person</th>\n",
       "      <th>Cleaned_Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Danielle Cohn</td>\n",
       "      <td>disgusting so u   and you had a first time thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Emily Ann Shaheen</td>\n",
       "      <td>im arab from iraq but my golden name is in eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Madeline &amp; Eric</td>\n",
       "      <td>your grocery carts are sooo small  ours look g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Madison Beer</td>\n",
       "      <td>be strong that s my fav song of demi lovato i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rebecca Black</td>\n",
       "      <td>my old girlfriend also had a book titled knot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Richard Gale</td>\n",
       "      <td>hes a bitch richard your a n idiot mate he bul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shannon Beveridge</td>\n",
       "      <td>my ex broke up with me because she wanted to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Steven Assanti</td>\n",
       "      <td>steven is a sagittarius  we be great is my guy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Person                                   Cleaned_Comments\n",
       "0      Danielle Cohn  disgusting so u   and you had a first time thi...\n",
       "1  Emily Ann Shaheen  im arab from iraq but my golden name is in eng...\n",
       "2    Madeline & Eric  your grocery carts are sooo small  ours look g...\n",
       "3       Madison Beer  be strong that s my fav song of demi lovato i ...\n",
       "4      Rebecca Black  my old girlfriend also had a book titled knot ...\n",
       "5       Richard Gale  hes a bitch richard your a n idiot mate he bul...\n",
       "6  Shannon Beveridge  my ex broke up with me because she wanted to b...\n",
       "7     Steven Assanti  steven is a sagittarius  we be great is my guy..."
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_clean= person_comments_grouped['Comments'].apply(text_cleaner).apply(emojiRemoval)\n",
    "               #.apply(lemmatizor).apply(stemmor).apply(spell_checker)\n",
    "\n",
    "person_comments_cleaned= person_comments_grouped.drop(['Comments'], axis=1)\n",
    "person_comments_cleaned.insert(1, 'Cleaned_Comments', comment_clean)\n",
    "\n",
    "person_comments_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'im arab from iraq but my golden name is in english   you remind me so much of     i hope to marry an arab woman one day  i never noticed how attractive i find them until i became an adult i love it  i sooooo have a necklace with my name in arabic and i m very proud of it  thank you for your videos  much love to you from another arab american  sister   i thought i d get some tips to look like an arabian girl   i m literally the hairiest fuckin girl in my class all arabs  and i can t do about it     except mustacge and brows i m not arab i m somali  my name is hard to say i have a gold necklace  it says my name on it   like if you got a gold  necklace wanna be a arab girl  hate men  abdullah ahmed more like  wanna be a brainwashed sjw  hate men where did you get your necklace    is middle east a new continent   since when the middle east became a new continent its not a continent    middle east is part of asia  i m arab algeriaaaaaa   and my face is covered with moles and i literally sho'"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at a sample text:\n",
    "person_comments_cleaned['Cleaned_Comments'][1][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickling cleaned data for later use\n",
    "person_comments_cleaned.to_pickle(\"person_comments_cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Danielle Cohn</th>\n",
       "      <td>disgusting so u   and you had a first time thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emily Ann Shaheen</th>\n",
       "      <td>im arab from iraq but my golden name is in eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madeline &amp; Eric</th>\n",
       "      <td>your grocery carts are sooo small  ours look g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madison Beer</th>\n",
       "      <td>be strong that s my fav song of demi lovato i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rebecca Black</th>\n",
       "      <td>my old girlfriend also had a book titled knot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Richard Gale</th>\n",
       "      <td>hes a bitch richard your a n idiot mate he bul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shannon Beveridge</th>\n",
       "      <td>my ex broke up with me because she wanted to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Steven Assanti</th>\n",
       "      <td>steven is a sagittarius  we be great is my guy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            comments\n",
       "Danielle Cohn      disgusting so u   and you had a first time thi...\n",
       "Emily Ann Shaheen  im arab from iraq but my golden name is in eng...\n",
       "Madeline & Eric    your grocery carts are sooo small  ours look g...\n",
       "Madison Beer       be strong that s my fav song of demi lovato i ...\n",
       "Rebecca Black      my old girlfriend also had a book titled knot ...\n",
       "Richard Gale       hes a bitch richard your a n idiot mate he bul...\n",
       "Shannon Beveridge  my ex broke up with me because she wanted to b...\n",
       "Steven Assanti     steven is a sagittarius  we be great is my guy..."
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting to Key Value pair\n",
    "personComments_dict = dict(zip(person_comments_cleaned.Person, person_comments_cleaned.Cleaned_Comments))\n",
    "\n",
    "# We are going to change this to key: person, comments: string format\n",
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ''.join(list_of_text)\n",
    "    return combined_text\n",
    "\n",
    "person_comments_cleaned_kvp = {key: [combine_text(value)] for (key, value) in personComments_dict.items()}\n",
    "\n",
    "personComments_corpus = pd.DataFrame.from_dict(person_comments_cleaned_kvp).transpose()\n",
    "personComments_corpus.columns = ['comments']\n",
    "personComments_corpus = personComments_corpus.sort_index()\n",
    "personComments_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickling corpus for later use\n",
    "personComments_corpus.to_pickle('personComments_corpus.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Term Matrix (DTM)\n",
    "\n",
    "Document Term Matrix is word counts in matrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aaaaaaaaaaaaa</th>\n",
       "      <th>aaaaaaakward</th>\n",
       "      <th>aaw</th>\n",
       "      <th>aawww</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abbara</th>\n",
       "      <th>...</th>\n",
       "      <th>يا</th>\n",
       "      <th>ياسمين</th>\n",
       "      <th>يتدلى</th>\n",
       "      <th>يديه</th>\n",
       "      <th>يستوعبها</th>\n",
       "      <th>يشبه</th>\n",
       "      <th>يعارض</th>\n",
       "      <th>ᴛʜɪs</th>\n",
       "      <th>ᴛʜᴜᴍʙɴᴀɪʟ</th>\n",
       "      <th>ᴡᴛғ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Danielle Cohn</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emily Ann Shaheen</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madeline &amp; Eric</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madison Beer</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rebecca Black</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Richard Gale</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shannon Beveridge</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Steven Assanti</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 7143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   aaa  aaaa  aaaaaaaaaaaaa  aaaaaaakward  aaw  aawww  ab  \\\n",
       "Danielle Cohn        0     0              0             0    0      0   0   \n",
       "Emily Ann Shaheen    0     0              0             0    0      1   0   \n",
       "Madeline & Eric      1     1              1             0    1      0   0   \n",
       "Madison Beer         0     0              0             0    0      0   0   \n",
       "Rebecca Black        0     0              0             0    0      0   0   \n",
       "Richard Gale         0     0              0             0    0      0   1   \n",
       "Shannon Beveridge    1     0              0             1    0      0   0   \n",
       "Steven Assanti       0     0              0             0    0      0   0   \n",
       "\n",
       "                   abandon  abandoned  abbara  ...  يا  ياسمين  يتدلى  يديه  \\\n",
       "Danielle Cohn            0          0       0  ...   0       0      0     0   \n",
       "Emily Ann Shaheen        0          0       2  ...   1       1      1     1   \n",
       "Madeline & Eric          1          0       0  ...   0       0      0     0   \n",
       "Madison Beer             0          0       0  ...   0       0      0     0   \n",
       "Rebecca Black            0          0       0  ...   0       0      0     0   \n",
       "Richard Gale             0          0       0  ...   0       0      0     0   \n",
       "Shannon Beveridge        0          0       0  ...   0       0      0     0   \n",
       "Steven Assanti           0          1       0  ...   0       0      0     0   \n",
       "\n",
       "                   يستوعبها  يشبه  يعارض  ᴛʜɪs  ᴛʜᴜᴍʙɴᴀɪʟ  ᴡᴛғ  \n",
       "Danielle Cohn             0     0      0     1          1    1  \n",
       "Emily Ann Shaheen         1     1      1     0          0    0  \n",
       "Madeline & Eric           0     0      0     0          0    0  \n",
       "Madison Beer              0     0      0     0          0    0  \n",
       "Rebecca Black             0     0      0     0          0    0  \n",
       "Richard Gale              0     0      0     0          0    0  \n",
       "Shannon Beveridge         0     0      0     0          0    0  \n",
       "Steven Assanti            0     0      0     0          0    0  \n",
       "\n",
       "[8 rows x 7143 columns]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating document-term matrix using CountVectorizer and excluding common English stop words\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(personComments_corpus.comments)\n",
    "personComments_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "personComments_dtm.index = personComments_corpus.index\n",
    "personComments_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "personComments_dtm.to_pickle(\"personComments_dtm.pkl\")\n",
    "pickle.dump(cv, open(\"cv.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
