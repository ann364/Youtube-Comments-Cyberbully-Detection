{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='http://pm1.narvii.com/6829/9ea1074cb40eb7b2111e9c4d2c662a35648f5796v2_00.jpg'/>\n",
    "\n",
    "<h1><center> Topic Modeling </center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling is looking at words in a text and determining what topic is the text all about. Here we will try to find topics using LDA (Latent Dirichlet Allocation).  We will go through several iterations in determining topics. For now, I will use parts of speech in choosing words in texts for the iterations.\n",
    "\n",
    "### Iteration 1: All Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Danielle Cohn</th>\n",
       "      <th>Emily Ann Shaheen</th>\n",
       "      <th>Madeline &amp; Eric</th>\n",
       "      <th>Madison Beer</th>\n",
       "      <th>Rebecca Black</th>\n",
       "      <th>Richard Gale</th>\n",
       "      <th>Shannon Beveridge</th>\n",
       "      <th>Steven Assanti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaa</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaa</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaaaaaaaaaa</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaaaakward</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaw</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Danielle Cohn  Emily Ann Shaheen  Madeline & Eric  \\\n",
       "aaa                        0                  0                1   \n",
       "aaaa                       0                  0                1   \n",
       "aaaaaaaaaaaaa              0                  0                1   \n",
       "aaaaaaakward               0                  0                0   \n",
       "aaw                        0                  0                1   \n",
       "\n",
       "               Madison Beer  Rebecca Black  Richard Gale  Shannon Beveridge  \\\n",
       "aaa                       0              0             0                  1   \n",
       "aaaa                      0              0             0                  0   \n",
       "aaaaaaaaaaaaa             0              0             0                  0   \n",
       "aaaaaaakward              0              0             0                  1   \n",
       "aaw                       0              0             0                  0   \n",
       "\n",
       "               Steven Assanti  \n",
       "aaa                         0  \n",
       "aaaa                        0  \n",
       "aaaaaaaaaaaaa               0  \n",
       "aaaaaaakward                0  \n",
       "aaw                         0  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Term Document Matrix (tdm) -Gensim needs transpose of dtm\n",
    "personComments_topic1 = pd.read_pickle('personComments_dtm2.pkl').transpose()\n",
    "personComments_topic1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting tdm into gensim format, \n",
    "# df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(personComments_topic1)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of the all terms and their respective location in the tdm\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA** (Latent Dirichlet Allocation)\n",
    "\n",
    "LDA is an example of topic model which builds a topic per document model and words per topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.021*\"friday\" + 0.011*\"guys\" + 0.008*\"song\" + 0.007*\"good\" + 0.007*\"cute\" + 0.007*\"baby\" + 0.005*\"girl\" + 0.005*\"gay\" + 0.004*\"time\" + 0.004*\"make\"'),\n",
       " (1,\n",
       "  '0.008*\"kid\" + 0.008*\"bully\" + 0.007*\"shit\" + 0.006*\"got\" + 0.006*\"fat\" + 0.006*\"little\" + 0.006*\"quot\" + 0.006*\"ass\" + 0.006*\"did\" + 0.006*\"fuck\"'),\n",
       " (2,\n",
       "  '0.020*\"voice\" + 0.010*\"sing\" + 0.010*\"beautiful\" + 0.009*\"good\" + 0.009*\"amazing\" + 0.007*\"hate\" + 0.007*\"pretty\" + 0.006*\"song\" + 0.006*\"better\" + 0.005*\"talented\"'),\n",
       " (3,\n",
       "  '0.030*\"arab\" + 0.014*\"girl\" + 0.009*\"lol\" + 0.007*\"gold\" + 0.006*\"omg\" + 0.006*\"arabs\" + 0.006*\"thanks\" + 0.006*\"thank\" + 0.006*\"arabic\" + 0.005*\"ann\"')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specifying LDA with the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=20)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 2: Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to return nouns from text\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Danielle Cohn</th>\n",
       "      <td>disgusting so u   and you had a first time thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emily Ann Shaheen</th>\n",
       "      <td>im arab from iraq but my golden name is in eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madeline &amp; Eric</th>\n",
       "      <td>your grocery carts are sooo small  ours look g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madison Beer</th>\n",
       "      <td>be strong that s my fav song of demi lovato i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rebecca Black</th>\n",
       "      <td>my old girlfriend also had a book titled knot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Richard Gale</th>\n",
       "      <td>hes a bitch richard your a n idiot mate he bul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shannon Beveridge</th>\n",
       "      <td>my ex broke up with me because she wanted to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Steven Assanti</th>\n",
       "      <td>steven is a sagittarius  we be great is my guy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            comments\n",
       "Danielle Cohn      disgusting so u   and you had a first time thi...\n",
       "Emily Ann Shaheen  im arab from iraq but my golden name is in eng...\n",
       "Madeline & Eric    your grocery carts are sooo small  ours look g...\n",
       "Madison Beer       be strong that s my fav song of demi lovato i ...\n",
       "Rebecca Black      my old girlfriend also had a book titled knot ...\n",
       "Richard Gale       hes a bitch richard your a n idiot mate he bul...\n",
       "Shannon Beveridge  my ex broke up with me because she wanted to b...\n",
       "Steven Assanti     steven is a sagittarius  we be great is my guy..."
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading Clean data\n",
    "personComments_corpus = pd.read_pickle('personComments_corpus.pkl')\n",
    "personComments_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Danielle Cohn</th>\n",
       "      <td>time video mom i i halo mates year y shoes bed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emily Ann Shaheen</th>\n",
       "      <td>arab iraq name woman day i i adult i sooooo ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madeline &amp; Eric</th>\n",
       "      <td>grocery carts ours family couple videooo cutee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madison Beer</th>\n",
       "      <td>fav song demi lovato i feels yesterday m madis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rebecca Black</th>\n",
       "      <td>girlfriend book knot t rope ya way friday rebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Richard Gale</th>\n",
       "      <td>hes bitch idiot mate someone deserve victim po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shannon Beveridge</th>\n",
       "      <td>ex m years anyone i d date lol babe personalit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Steven Assanti</th>\n",
       "      <td>steven sagittarius guy shit ass m years man ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            comments\n",
       "Danielle Cohn      time video mom i i halo mates year y shoes bed...\n",
       "Emily Ann Shaheen  arab iraq name woman day i i adult i sooooo ne...\n",
       "Madeline & Eric    grocery carts ours family couple videooo cutee...\n",
       "Madison Beer       fav song demi lovato i feels yesterday m madis...\n",
       "Rebecca Black      girlfriend book knot t rope ya way friday rebe...\n",
       "Richard Gale       hes bitch idiot mate someone deserve victim po...\n",
       "Shannon Beveridge  ex m years anyone i d date lol babe personalit...\n",
       "Steven Assanti     steven sagittarius guy shit ass m years man ma..."
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering noun from clean data\n",
    "personComments_nouns = pd.DataFrame(personComments_corpus.comments.apply(nouns) )\n",
    "personComments_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aaaaaaaaaaaaa</th>\n",
       "      <th>aaaaaaakward</th>\n",
       "      <th>aawww</th>\n",
       "      <th>abbara</th>\n",
       "      <th>abdulla</th>\n",
       "      <th>abidal</th>\n",
       "      <th>ability</th>\n",
       "      <th>abit</th>\n",
       "      <th>abortion</th>\n",
       "      <th>...</th>\n",
       "      <th>ووهي</th>\n",
       "      <th>يا</th>\n",
       "      <th>ياسمين</th>\n",
       "      <th>يتدلى</th>\n",
       "      <th>يديه</th>\n",
       "      <th>يستوعبها</th>\n",
       "      <th>يشبه</th>\n",
       "      <th>يعارض</th>\n",
       "      <th>ᴛʜᴜᴍʙɴᴀɪʟ</th>\n",
       "      <th>ᴡᴛғ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Danielle Cohn</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emily Ann Shaheen</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madeline &amp; Eric</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madison Beer</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rebecca Black</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Richard Gale</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shannon Beveridge</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Steven Assanti</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 4310 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   aaaa  aaaaaaaaaaaaa  aaaaaaakward  aawww  abbara  abdulla  \\\n",
       "Danielle Cohn         0              0             0      0       0        0   \n",
       "Emily Ann Shaheen     0              0             0      1       2        1   \n",
       "Madeline & Eric       1              1             0      0       0        0   \n",
       "Madison Beer          0              0             0      0       0        0   \n",
       "Rebecca Black         0              0             0      0       0        0   \n",
       "Richard Gale          0              0             0      0       0        0   \n",
       "Shannon Beveridge     0              0             1      0       0        0   \n",
       "Steven Assanti        0              0             0      0       0        0   \n",
       "\n",
       "                   abidal  ability  abit  abortion  ...  ووهي  يا  ياسمين  \\\n",
       "Danielle Cohn           0        0     1         0  ...     0   0       0   \n",
       "Emily Ann Shaheen       1        0     0         0  ...     1   1       1   \n",
       "Madeline & Eric         0        0     0         1  ...     0   0       0   \n",
       "Madison Beer            0        1     0         0  ...     0   0       0   \n",
       "Rebecca Black           0        1     0         0  ...     0   0       0   \n",
       "Richard Gale            0        0     0         1  ...     0   0       0   \n",
       "Shannon Beveridge       0        1     0         0  ...     0   0       0   \n",
       "Steven Assanti          0        0     0         0  ...     0   0       0   \n",
       "\n",
       "                   يتدلى  يديه  يستوعبها  يشبه  يعارض  ᴛʜᴜᴍʙɴᴀɪʟ  ᴡᴛғ  \n",
       "Danielle Cohn          0     0         0     0      0          1    1  \n",
       "Emily Ann Shaheen      1     1         1     1      1          0    0  \n",
       "Madeline & Eric        0     0         0     0      0          0    0  \n",
       "Madison Beer           0     0         0     0      0          0    0  \n",
       "Rebecca Black          0     0         0     0      0          0    0  \n",
       "Richard Gale           0     0         0     0      0          0    0  \n",
       "Shannon Beveridge      0     0         0     0      0          0    0  \n",
       "Steven Assanti         0     0         0     0      0          0    0  \n",
       "\n",
       "[8 rows x 4310 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'just', 'don', 'br', 'video', 'know', 'people', 'love', 'really', 'dani', \n",
    " 'danielle', 'madison', 'rebecca', 'shannon', 'really', 'richard', 'casey', 'emily', 'steven']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreating Document-Term matrix with only nouns\n",
    "\n",
    "cv_noun = CountVectorizer(stop_words=stop_words)\n",
    "data_cv_noun = cv_noun.fit_transform(personComments_nouns.comments)\n",
    "personComments_dtm_nouns= pd.DataFrame(data_cv_noun.toarray(), columns=cv_noun.get_feature_names())\n",
    "personComments_dtm_nouns.index = personComments_nouns.index\n",
    "personComments_dtm_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the gensim corpus with only noun\n",
    "corpus_noun = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(personComments_dtm_nouns.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2word_noun = dict((v, k) for k, v in cv_noun.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.013*\"voice\" + 0.012*\"kid\" + 0.008*\"time\" + 0.008*\"years\" + 0.007*\"girl\" + 0.007*\"life\" + 0.007*\"way\" + 0.007*\"quot\" + 0.006*\"ass\" + 0.005*\"guys\"'),\n",
       " (1,\n",
       "  '0.023*\"song\" + 0.020*\"friday\" + 0.009*\"youtube\" + 0.009*\"fun\" + 0.007*\"music\" + 0.007*\"guy\" + 0.007*\"https\" + 0.007*\"watch\" + 0.007*\"ass\" + 0.006*\"www\"'),\n",
       " (2,\n",
       "  '0.017*\"baby\" + 0.017*\"girl\" + 0.014*\"guys\" + 0.011*\"family\" + 0.008*\"arab\" + 0.008*\"parents\" + 0.007*\"school\" + 0.007*\"thanks\" + 0.007*\"lol\" + 0.007*\"time\"'),\n",
       " (3,\n",
       "  '0.000*\"girl\" + 0.000*\"guys\" + 0.000*\"lol\" + 0.000*\"life\" + 0.000*\"ass\" + 0.000*\"kid\" + 0.000*\"time\" + 0.000*\"shit\" + 0.000*\"guy\" + 0.000*\"voice\"')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating LDA with nouns with 4 topics\n",
    "lda_noun = models.LdaModel(corpus=corpus_noun, num_topics=4, id2word=id2word_noun, passes=20)\n",
    "lda_noun.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 3: Nouns and Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to return nouns and adjectives from text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Danielle Cohn</th>\n",
       "      <td>u first time video mom worst i mate i halo mat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emily Ann Shaheen</th>\n",
       "      <td>im arab iraq golden name english much i arab w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madeline &amp; Eric</th>\n",
       "      <td>grocery carts sooo small ours gigantic family ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madison Beer</th>\n",
       "      <td>strong s fav song demi lovato i feels yesterda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rebecca Black</th>\n",
       "      <td>old girlfriend book knot t rope ya long way fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Richard Gale</th>\n",
       "      <td>hes bitch n idiot mate someone t deserve victi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shannon Beveridge</th>\n",
       "      <td>ex lesbian m years anyone beautiful i d date l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Steven Assanti</th>\n",
       "      <td>steven sagittarius great guy alive s gross shi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            comments\n",
       "Danielle Cohn      u first time video mom worst i mate i halo mat...\n",
       "Emily Ann Shaheen  im arab iraq golden name english much i arab w...\n",
       "Madeline & Eric    grocery carts sooo small ours gigantic family ...\n",
       "Madison Beer       strong s fav song demi lovato i feels yesterda...\n",
       "Rebecca Black      old girlfriend book knot t rope ya long way fr...\n",
       "Richard Gale       hes bitch n idiot mate someone t deserve victi...\n",
       "Shannon Beveridge  ex lesbian m years anyone beautiful i d date l...\n",
       "Steven Assanti     steven sagittarius great guy alive s gross shi..."
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying nouns_adj functions to filter nouns and adjective\n",
    "personComments_nounsAdj = pd.DataFrame(personComments_corpus.comments.apply(nouns_adj))\n",
    "personComments_nounsAdj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aaaaaaaaaaaaa</th>\n",
       "      <th>aaaaaaakward</th>\n",
       "      <th>aawww</th>\n",
       "      <th>abbara</th>\n",
       "      <th>abdoh</th>\n",
       "      <th>abdulla</th>\n",
       "      <th>abidal</th>\n",
       "      <th>ability</th>\n",
       "      <th>...</th>\n",
       "      <th>يا</th>\n",
       "      <th>ياسمين</th>\n",
       "      <th>يتدلى</th>\n",
       "      <th>يديه</th>\n",
       "      <th>يستوعبها</th>\n",
       "      <th>يشبه</th>\n",
       "      <th>يعارض</th>\n",
       "      <th>ᴛʜɪs</th>\n",
       "      <th>ᴛʜᴜᴍʙɴᴀɪʟ</th>\n",
       "      <th>ᴡᴛғ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Danielle Cohn</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emily Ann Shaheen</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madeline &amp; Eric</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madison Beer</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rebecca Black</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Richard Gale</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shannon Beveridge</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Steven Assanti</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 5270 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   aaa  aaaa  aaaaaaaaaaaaa  aaaaaaakward  aawww  abbara  \\\n",
       "Danielle Cohn        0     0              0             0      0       0   \n",
       "Emily Ann Shaheen    0     0              0             0      1       2   \n",
       "Madeline & Eric      1     1              1             0      0       0   \n",
       "Madison Beer         0     0              0             0      0       0   \n",
       "Rebecca Black        0     0              0             0      0       0   \n",
       "Richard Gale         0     0              0             0      0       0   \n",
       "Shannon Beveridge    0     0              0             1      0       0   \n",
       "Steven Assanti       0     0              0             0      0       0   \n",
       "\n",
       "                   abdoh  abdulla  abidal  ability  ...  يا  ياسمين  يتدلى  \\\n",
       "Danielle Cohn          0        0       0        0  ...   0       0      0   \n",
       "Emily Ann Shaheen      1        1       1        0  ...   1       1      1   \n",
       "Madeline & Eric        0        0       0        0  ...   0       0      0   \n",
       "Madison Beer           0        0       0        1  ...   0       0      0   \n",
       "Rebecca Black          0        0       0        1  ...   0       0      0   \n",
       "Richard Gale           0        0       0        0  ...   0       0      0   \n",
       "Shannon Beveridge      0        0       0        1  ...   0       0      0   \n",
       "Steven Assanti         0        0       0        0  ...   0       0      0   \n",
       "\n",
       "                   يديه  يستوعبها  يشبه  يعارض  ᴛʜɪs  ᴛʜᴜᴍʙɴᴀɪʟ  ᴡᴛғ  \n",
       "Danielle Cohn         0         0     0      0     1          1    1  \n",
       "Emily Ann Shaheen     1         1     1      1     0          0    0  \n",
       "Madeline & Eric       0         0     0      0     0          0    0  \n",
       "Madison Beer          0         0     0      0     0          0    0  \n",
       "Rebecca Black         0         0     0      0     0          0    0  \n",
       "Richard Gale          0         0     0      0     0          0    0  \n",
       "Shannon Beveridge     0         0     0      0     0          0    0  \n",
       "Steven Assanti        0         0     0      0     0          0    0  \n",
       "\n",
       "[8 rows x 5270 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cv_nouns_adj = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cv_nouns_adj = cv_nouns_adj.fit_transform(personComments_nounsAdj.comments)\n",
    "personComments_dtm_nounsAdj = pd.DataFrame(data_cv_nouns_adj.toarray(), columns=cv_nouns_adj.get_feature_names())\n",
    "personComments_dtm_nounsAdj.index = personComments_nounsAdj.index\n",
    "personComments_dtm_nounsAdj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpus_nouns_adj = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(personComments_dtm_nounsAdj.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2word_nouns_adj = dict((v, k) for k, v in cv_nouns_adj.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.095*\"friday\" + 0.033*\"song\" + 0.009*\"music\" + 0.007*\"david\" + 0.007*\"voice\" + 0.006*\"great\" + 0.006*\"weekend\" + 0.006*\"ta\" + 0.006*\"black\" + 0.005*\"partyin\"'),\n",
       " (1,\n",
       "  '0.036*\"voice\" + 0.017*\"beautiful\" + 0.011*\"song\" + 0.009*\"amazing\" + 0.008*\"singer\" + 0.007*\"famous\" + 0.006*\"haters\" + 0.006*\"shes\" + 0.005*\"perfect\" + 0.005*\"opinion\"'),\n",
       " (2,\n",
       "  '0.012*\"kid\" + 0.009*\"gay\" + 0.009*\"fat\" + 0.007*\"lesbian\" + 0.007*\"mikey\" + 0.006*\"friend\" + 0.006*\"https\" + 0.005*\"sex\" + 0.005*\"victim\" + 0.005*\"bitch\"'),\n",
       " (3,\n",
       "  '0.029*\"arab\" + 0.015*\"cute\" + 0.010*\"family\" + 0.008*\"beautiful\" + 0.007*\"gold\" + 0.007*\"pregnant\" + 0.007*\"thanks\" + 0.006*\"arabs\" + 0.006*\"girls\" + 0.006*\"arabic\"')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA topic modeling with 4 topics\n",
    "lda_nouns_adj = models.LdaModel(corpus=corpus_nouns_adj, num_topics=4, id2word=id2word_nouns_adj, passes=80)\n",
    "lda_nouns_adj.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'Danielle Cohn'),\n",
       " (3, 'Emily Ann Shaheen'),\n",
       " (3, 'Madeline & Eric'),\n",
       " (1, 'Madison Beer'),\n",
       " (0, 'Rebecca Black'),\n",
       " (2, 'Richard Gale'),\n",
       " (2, 'Shannon Beveridge'),\n",
       " (2, 'Steven Assanti')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics comments for each uploader contains\n",
    "corpus_transformed = lda_nouns_adj[corpus_nouns_adj]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], personComments_dtm_nounsAdj.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics and People\n",
    "\n",
    "\n",
    "   \n",
    "* **Topic 0:** song/music/voice, Friday/party/weekend\n",
    "\n",
    "  Rebecca Black   \n",
    "    \n",
    "    \n",
    "* **Topic 1:** beautiful/perfect/amazing song/music/voice \n",
    "\n",
    "  Madison Beer\n",
    "\n",
    "\n",
    "* **Topic 2:** gay/lesbian, sex, kid, fat, victim, b**ch\n",
    "\n",
    "  Shannon Beveridge, Steven Assanti, Richard Gale\n",
    " \n",
    " \n",
    " * **Topic 3:** arab, gold, girls, family, beautiful, pregnant\n",
    "\n",
    "    Emily Ann Shaheen, Madeline & Eric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights\n",
    "\n",
    "From topics people talk about in the comments for each of the people , we can see that yet **Shannon Beveridge, Steven Assanti, Richard Gale** are prone to harsh comments than any other people in the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
